<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Federated LLM Learning - Meta-Learning via RL</title>
    <style>
        body {
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #fff;
            font-family: system-ui;
        }
        #app {
            max-width: 1800px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 16px;
            padding: 30px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            color: #333;
        }
        .badge {
            display: inline-block;
            background: #8b5cf6;
            color: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
            margin: 5px 5px 10px 0;
        }
        .info-box {
            background: #f0f9ff;
            border-left: 4px solid #3b82f6;
            padding: 15px;
            margin: 20px 0;
            border-radius: 8px;
        }
        .client-panel {
            background: #f9fafb;
            border: 2px solid #e5e7eb;
            border-radius: 12px;
            padding: 15px;
            margin-bottom: 15px;
        }
        .llm-response {
            background: #fff;
            border: 1px solid #d1d5db;
            border-radius: 8px;
            padding: 12px;
            margin: 10px 0;
            font-family: monospace;
            font-size: 12px;
            max-height: 100px;
            overflow-y: auto;
        }
        .success { color: #22c55e; font-weight: bold; }
        .error { color: #ef4444; font-weight: bold; }
        code {
            background: #f3f4f6;
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 11px;
        }
    </style>
</head>
<body>
    <div id="app"></div>

    <script type="module">
        import { createFederatedApp } from '../components/app-template.js';

        // üß† FEDERATED LLM LEARNING - Meta-Learning via RL!
        // Each client uses Ollama LLM + RL to learn optimal prompting strategies
        // Then federates this knowledge across all clients

        // Trivia questions dataset
        const TRIVIA_QUESTIONS = [
            { q: "What is the capital of France?", a: "Paris", category: "geography" },
            { q: "What is 2 + 2?", a: "4", category: "math" },
            { q: "Who wrote Romeo and Juliet?", a: "Shakespeare", category: "literature" },
            { q: "What is the largest planet in our solar system?", a: "Jupiter", category: "science" },
            { q: "In what year did World War 2 end?", a: "1945", category: "history" },
            { q: "What is the chemical symbol for water?", a: "H2O", category: "science" },
            { q: "What is the capital of Japan?", a: "Tokyo", category: "geography" },
            { q: "What is 10 * 10?", a: "100", category: "math" },
            { q: "Who painted the Mona Lisa?", a: "Leonardo da Vinci", category: "art" },
            { q: "What is the speed of light?", a: "299792458", category: "science" }
        ];

        // Prompt strategies (actions)
        const PROMPT_STRATEGIES = [
            { name: "Direct", template: (q) => `${q}\nAnswer:` },
            { name: "Think", template: (q) => `${q}\nLet's think step by step and then provide the answer:` },
            { name: "Concise", template: (q) => `${q}\nGive only the answer, nothing else:` },
            { name: "Expert", template: (q) => `You are an expert. ${q}\nAnswer:` },
            { name: "Format", template: (q) => `Question: ${q}\nProvide a brief, accurate answer:\nAnswer:` }
        ];

        // Ollama API configuration
        const OLLAMA_CONFIG = {
            baseURL: 'http://localhost:11434',
            model: 'qwen3:1.7b', // Fast 2B model
            options: {
                temperature: 0.1,
                num_predict: 50,
                num_thread: 4 // Optimize for parallel requests
            },
            timeout: 10000 // 10 second timeout (Ollama typically responds in ~350ms)
        };

        // Check if Ollama is available
        async function checkOllamaAvailability() {
            try {
                console.log('üîç Checking Ollama availability...');
                const response = await fetch(`${OLLAMA_CONFIG.baseURL}/api/tags`, {
                    signal: AbortSignal.timeout(5000)
                });
                const isAvailable = response.ok;
                console.log(isAvailable ? '‚úÖ Ollama is available' : '‚ùå Ollama not responding');
                return isAvailable;
            } catch (e) {
                console.error('‚ùå Ollama check failed:', e.message);
                return false;
            }
        }

        // Call Ollama LLM (NO MOCK - Real only!)
        const MAX_RETRIES = 2;
        const RETRY_DELAY_BASE = 1000; // Start with 1 second
        
        async function callLLM(prompt, question, retryCount = 0) {
            const startTime = Date.now();
            
            try {
                console.log(`üì§ [Attempt ${retryCount + 1}] Sending to LLM (${question.category}): ${question.q.substring(0, 50)}...`);
                
                const controller = new AbortController();
                const timeoutId = setTimeout(() => controller.abort(), OLLAMA_CONFIG.timeout);
                
                const response = await fetch(`${OLLAMA_CONFIG.baseURL}/api/generate`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        model: OLLAMA_CONFIG.model,
                        prompt: prompt,
                        stream: false,
                        options: {
                            ...OLLAMA_CONFIG.options,
                            num_ctx: 2048 // Limit context to avoid memory issues
                        }
                    }),
                    signal: controller.signal
                });
                
                clearTimeout(timeoutId);
                
                if (!response.ok) {
                    const errorText = await response.text();
                    const error = new Error(`HTTP ${response.status}: ${errorText}`);
                    
                    // Retry on 5xx errors or 429 (rate limit)
                    if ((response.status >= 500 || response.status === 429) && retryCount < MAX_RETRIES) {
                        const delay = RETRY_DELAY_BASE * Math.pow(2, retryCount);
                        console.warn(`‚ö†Ô∏è Retrying after ${delay}ms (HTTP ${response.status})`);
                        await new Promise(resolve => setTimeout(resolve, delay));
                        return callLLM(prompt, question, retryCount + 1);
                    }
                    
                    throw error;
                }
                
                const data = await response.json();
                const elapsed = Date.now() - startTime;
                
                console.log(`üì• LLM Response (${elapsed}ms): ${data.response.trim().substring(0, 80)}...`);
                
                return data.response.trim();
            } catch (e) {
                const elapsed = Date.now() - startTime;
                
                // Retry on network errors or timeouts
                if ((e.name === 'AbortError' || e.name === 'TypeError') && retryCount < MAX_RETRIES) {
                    const delay = RETRY_DELAY_BASE * Math.pow(2, retryCount);
                    console.warn(`‚ö†Ô∏è Retrying after ${delay}ms due to: ${e.message}`);
                    await new Promise(resolve => setTimeout(resolve, delay));
                    return callLLM(prompt, question, retryCount + 1);
                }
                
                console.error(`‚ùå LLM call FAILED after ${elapsed}ms (${retryCount + 1} attempts):`, e.message);
                
                if (e.name === 'AbortError') {
                    throw new Error(`Timeout after ${OLLAMA_CONFIG.timeout}ms - Ollama overloaded or stuck`);
                }
                throw new Error(`${e.message}`);
            }
        }

        // Evaluate answer quality (simple string matching)
        function evaluateAnswer(llmResponse, correctAnswer) {
            const response = llmResponse.toLowerCase().trim();
            const answer = correctAnswer.toLowerCase().trim();
            
            // Direct match
            if (response === answer) return 1.0;
            
            // Contains the answer
            if (response.includes(answer)) return 0.8;
            
            // Partial match (first few chars)
            if (answer.length > 3 && response.startsWith(answer.substring(0, 3))) return 0.5;
            
            // Wrong
            return 0.0;
        }

        // Log configuration
        console.log('‚öôÔ∏è CONFIGURATION:');
        console.log('  Model:', OLLAMA_CONFIG.model);
        console.log('  Clients: 4 (parallel execution)');
        console.log('  Timeout:', OLLAMA_CONFIG.timeout + 'ms');
        console.log('  Max Retries:', MAX_RETRIES);
        console.log('  Context:', OLLAMA_CONFIG.options.num_ctx || 2048);
        console.log('  Threads:', OLLAMA_CONFIG.options.num_thread || 'default');
        
        // Create the federated LLM learning system
        createFederatedApp({
            name: 'üß† Federated LLM Meta-Learning',
            subtitle: 'ü§ñ Each client uses Ollama LLM ‚Ä¢ üéØ RL learns optimal prompting ‚Ä¢ üîÑ Federation shares strategies',
            
            numClients: 4, // 4 clients running in parallel
            canvasWidth: 400,
            canvasHeight: 300,
            
            // RL parameters
            alpha: 0.2,
            gamma: 0.95,
            epsilon: 0.3,
            epsilonDecay: 0.98,
            minEpsilon: 0.1,
            
            // Federation
            autoFederate: false,
            federationInterval: 10, // Federate every 10 questions
            
            environment: {
                actions: PROMPT_STRATEGIES.map(s => s.name),
                
                // State: Question category
                getState: (state) => {
                    return state.currentQuestion.category;
                },
                
                // Step: Ask LLM a question using selected prompt strategy
                step: async (state, action) => {
                    const strategy = PROMPT_STRATEGIES[action];
                    const question = state.currentQuestion;
                    
                    // Build prompt
                    const prompt = strategy.template(question.q);
                    
                    let llmResponse = '';
                    let score = 0;
                    let reward = -10; // Default penalty
                    
                    try {
                        // Call real LLM - let it fail naturally if Ollama is down
                        llmResponse = await callLLM(prompt, question);
                        
                        // Evaluate answer
                        score = evaluateAnswer(llmResponse, question.a);
                        reward = score * 10 - 1; // -1 to 9 scale
                        
                        // Track successful question
                        if (score >= 0.8) state.correctAnswers++;
                        
                        // Mark as available if successful
                        state.ollamaAvailable = true;
                        
                    } catch (error) {
                        console.error(`‚ùå Step failed for client ${state.clientId}:`, error.message);
                        llmResponse = `ERROR: ${error.message}`;
                        score = 0;
                        reward = -10; // Heavy penalty for errors
                        state.errorCount = (state.errorCount || 0) + 1;
                        state.ollamaAvailable = false; // Mark as unavailable on error
                    }
                    
                    // Track total questions (success or failure)
                    state.totalQuestions++;
                    
                    // Store result
                    state.lastPrompt = prompt;
                    state.lastResponse = llmResponse;
                    state.lastScore = score;
                    state.lastStrategy = strategy.name;
                    
                    // Get next question
                    const nextQuestion = TRIVIA_QUESTIONS[
                        Math.floor(Math.random() * TRIVIA_QUESTIONS.length)
                    ];
                    
                    return {
                        state: {
                            ...state,
                            currentQuestion: nextQuestion,
                            steps: state.steps + 1
                        },
                        reward,
                        done: state.steps >= 20 // 20 questions per episode
                    };
                },
                
                // Reset: Start with a random question
                reset: (clientId) => {
                    const question = TRIVIA_QUESTIONS[
                        Math.floor(Math.random() * TRIVIA_QUESTIONS.length)
                    ];
                    
                    return {
                        currentQuestion: question,
                        lastPrompt: '',
                        lastResponse: '',
                        lastScore: 0,
                        lastStrategy: '',
                        totalQuestions: 0,
                        correctAnswers: 0,
                        errorCount: 0,
                        steps: 0,
                        clientId: clientId || 0,
                        ollamaAvailable: false
                    };
                }
            },
            
            // Custom rendering for LLM demo
            render: (ctx, state) => {
                const w = ctx.canvas.width;
                const h = ctx.canvas.height;
                
                // Clear
                ctx.fillStyle = '#f9fafb';
                ctx.fillRect(0, 0, w, h);
                
                // Title
                ctx.fillStyle = '#1f2937';
                ctx.font = 'bold 14px system-ui';
                ctx.fillText('üß† LLM Agent', 10, 25);
                
                // Status indicator
                ctx.fillStyle = state.ollamaAvailable ? '#22c55e' : '#ef4444';
                ctx.font = '10px system-ui';
                ctx.fillText(
                    state.ollamaAvailable ? '‚úì Live' : '‚úó Error',
                    w - 50,
                    25
                );
                
                // Error count if any
                if (state.errorCount > 0) {
                    ctx.fillStyle = '#ef4444';
                    ctx.fillText(`‚ö† ${state.errorCount} errs`, w - 60, 40);
                }
                
                // Question
                ctx.font = '12px system-ui';
                ctx.fillStyle = '#4b5563';
                ctx.fillText('Question:', 10, 50);
                
                ctx.fillStyle = '#1f2937';
                ctx.font = 'bold 11px system-ui';
                const q = state.currentQuestion.q;
                const maxWidth = w - 20;
                const lines = [];
                let currentLine = '';
                q.split(' ').forEach(word => {
                    const testLine = currentLine + word + ' ';
                    const metrics = ctx.measureText(testLine);
                    if (metrics.width > maxWidth && currentLine) {
                        lines.push(currentLine);
                        currentLine = word + ' ';
                    } else {
                        currentLine = testLine;
                    }
                });
                lines.push(currentLine);
                lines.forEach((line, i) => {
                    ctx.fillText(line, 10, 70 + i * 15);
                });
                
                // Category badge
                ctx.fillStyle = '#8b5cf6';
                ctx.fillRect(10, 110, 80, 20);
                ctx.fillStyle = '#fff';
                ctx.font = 'bold 10px system-ui';
                ctx.fillText(state.currentQuestion.category.toUpperCase(), 15, 123);
                
                // Last strategy used
                if (state.lastStrategy) {
                    ctx.fillStyle = '#4b5563';
                    ctx.font = '11px system-ui';
                    ctx.fillText(`Strategy: ${state.lastStrategy}`, 10, 150);
                }
                
                // Last response
                if (state.lastResponse) {
                    ctx.fillStyle = '#6b7280';
                    ctx.font = '10px monospace';
                    const response = state.lastResponse.substring(0, 100) + 
                                   (state.lastResponse.length > 100 ? '...' : '');
                    
                    const respLines = [];
                    let respLine = '';
                    response.split(' ').forEach(word => {
                        const testLine = respLine + word + ' ';
                        if (ctx.measureText(testLine).width > maxWidth && respLine) {
                            respLines.push(respLine);
                            respLine = word + ' ';
                        } else {
                            respLine = testLine;
                        }
                    });
                    respLines.push(respLine);
                    
                    ctx.fillText('Response:', 10, 175);
                    respLines.forEach((line, i) => {
                        ctx.fillText(line, 10, 190 + i * 12);
                    });
                }
                
                // Score indicator
                if (state.lastScore > 0) {
                    ctx.fillStyle = state.lastScore >= 0.8 ? '#22c55e' : '#ef4444';
                    ctx.fillRect(w - 80, 10, 70, 30);
                    ctx.fillStyle = '#fff';
                    ctx.font = 'bold 12px system-ui';
                    ctx.fillText(
                        state.lastScore >= 0.8 ? '‚úì Correct' : '‚úó Wrong',
                        w - 72,
                        28
                    );
                }
                
                // Stats
                ctx.fillStyle = '#1f2937';
                ctx.font = '11px system-ui';
                
                const accuracy = state.totalQuestions > 0 ? 
                    Math.round(state.correctAnswers / state.totalQuestions * 100) : 0;
                const errorRate = state.totalQuestions > 0 ?
                    Math.round((state.errorCount || 0) / state.totalQuestions * 100) : 0;
                
                ctx.fillText(
                    `Accuracy: ${accuracy}% | Errors: ${errorRate}%`,
                    10,
                    h - 20
                );
                ctx.fillText(
                    `Questions: ${state.totalQuestions} | Errors: ${state.errorCount || 0}`,
                    10,
                    h - 5
                );
            },
            
            // Lifecycle hooks
            onClientInit: async (client) => {
                console.log(`üîß Client ${client.id} initializing...`);
                const available = await checkOllamaAvailability();
                client.state.ollamaAvailable = available;
                window.ollamaDetected = available; // For status display
                
                if (!available) {
                    console.error(`‚ùå Client ${client.id}: Ollama NOT available!`);
                    console.error('   Make sure Ollama is running and qwen3:1.7b is installed');
                } else {
                    console.log(`‚úÖ Client ${client.id}: Ollama connected (qwen3:1.7b ready)`);
                }
            },
            
            onEpisodeEnd: (client) => {
                const accuracy = client.state.totalQuestions > 0 ?
                    (client.state.correctAnswers / client.state.totalQuestions * 100) : 0;
                const errorCount = client.state.errorCount || 0;
                
                console.log(`üìä Client ${client.id} episode complete: ${accuracy.toFixed(0)}% accuracy | ${errorCount} errors`);
            },
            
            onFederation: (globalModel, round) => {
                const strategies = Object.keys(globalModel);
                console.log(`‚úÖ Federation ${round}: Shared strategies for ${strategies.length} question types`);
                
                // Show which strategies work best
                strategies.forEach(category => {
                    const qValues = globalModel[category];
                    const bestStrategy = qValues.indexOf(Math.max(...qValues));
                    console.log(`  ${category}: Best strategy = ${PROMPT_STRATEGIES[bestStrategy].name}`);
                });
            }
        });

        // Add instructions
        setTimeout(() => {
            const instructionsDiv = document.createElement('div');
            instructionsDiv.className = 'info-box';
            instructionsDiv.innerHTML = `
                <h3 style="margin-top:0">üß† Federated LLM Meta-Learning - Real Mode Only</h3>
                <p><strong>‚úÖ Status:</strong> ${window.ollamaDetected ? 'üü¢ Ollama Connected (qwen3:1.7b)' : 'üî¥ Ollama Not Detected'}</p>
                <p style="margin-top:10px"><strong>üéÆ Requirements:</strong></p>
                <p style="margin-left:20px">‚úÖ Ollama must be running</p>
                <p style="margin-left:20px">‚úÖ Model <code>qwen3:1.7b</code> must be installed</p>
                <p style="margin-left:20px">‚úÖ Check console (F12) for detailed logs</p>
                <hr style="margin: 15px 0">
                <h4>üéØ What's Happening:</h4>
                <ul style="margin:0;padding-left:20px">
                    <li><strong>State:</strong> Question category (geography, math, science, etc.)</li>
                    <li><strong>Actions:</strong> 5 different prompt strategies (Direct, Think, Concise, Expert, Format)</li>
                    <li><strong>Reward:</strong> Based on LLM answer correctness (0-10 scale)</li>
                    <li><strong>Learning:</strong> RL discovers which prompts work best for each category</li>
                    <li><strong>Federation:</strong> Clients share learned prompting strategies</li>
                </ul>
                <p style="margin-top:15px"><strong>üí° Meta-Learning:</strong> The RL agent learns <em>how to use</em> the LLM effectively!</p>
                <p style="margin-top:10px"><strong>‚ö†Ô∏è Note:</strong> No mock mode - all calls go to real LLM. Watch console for detailed logs.</p>
            `;
            
            const app = document.getElementById('app');
            app.insertBefore(instructionsDiv, app.firstChild.nextSibling);
        }, 100);
    </script>
</body>
</html>

