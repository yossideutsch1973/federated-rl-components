# Convergence Acceleration Techniques for Tabular Q-Learning

## ðŸ“Š Current Performance

**Magnet Slalom:**
- State space: 300 states
- Action space: 27 actions
- Q-table: 8,100 values
- Convergence: ~500-1000 episodes (estimated)
- Issue: Sparse rewards (only at flags)

---

## ðŸš€ Methods to Accelerate Convergence

### **1. Adaptive Learning Rate (Î±)**

#### **A) State-Visit Based (Theoretically Optimal)**
```javascript
// Robbins-Monro conditions: Î£Î± = âˆž, Î£Î±Â² < âˆž
// Ensures convergence to optimal Q*

const stateVisits = {};  // Track visits per state

function getAlpha(state, baseAlpha = 0.5) {
    if (!stateVisits[state]) stateVisits[state] = 0;
    stateVisits[state]++;
    
    // Î±_s = Î±_0 / (1 + visits_s^0.6)
    return baseAlpha / (1 + Math.pow(stateVisits[state], 0.6));
}

// Early: Î± â‰ˆ 0.5 (fast learning)
// After 100 visits: Î± â‰ˆ 0.08 (stable)
// After 1000 visits: Î± â‰ˆ 0.02 (converged)
```

**Pros:**
- Theoretically sound
- Fast early learning
- Stable convergence
- No hyperparameter tuning

**Cons:**
- Requires tracking per-state visits

#### **B) Episode-Based Decay**
```javascript
function getAlpha(episode, alpha0 = 0.3) {
    // Two-stage decay
    if (episode < 200) {
        // Fast decay during exploration phase
        return alpha0 * Math.pow(0.95, episode / 20);
    } else {
        // Slow decay during exploitation phase
        return 0.05 * Math.pow(0.998, episode - 200);
    }
}

// Episode 0: Î± = 0.30
// Episode 100: Î± = 0.12
// Episode 200: Î± = 0.05
// Episode 500: Î± = 0.04
```

**Pros:**
- Simple to implement
- Works well in practice

**Cons:**
- Requires tuning decay schedule

#### **C) Performance-Based**
```javascript
let recentRewards = [];
let currentAlpha = 0.3;

function adaptAlpha(episodeReward) {
    recentRewards.push(episodeReward);
    if (recentRewards.length > 50) recentRewards.shift();
    
    if (recentRewards.length >= 50) {
        const recent = recentRewards.slice(-25);
        const older = recentRewards.slice(0, 25);
        const improvement = average(recent) - average(older);
        
        if (improvement < 1.0) {
            // Learning plateaued, reduce Î±
            currentAlpha *= 0.9;
        }
    }
    
    return Math.max(0.01, currentAlpha);
}
```

**Pros:**
- Adapts to actual learning progress

**Cons:**
- More complex
- May be noisy

---

### **2. Optimistic Initialization**

```javascript
// Standard: Initialize Q(s,a) = 0
// Optimistic: Initialize Q(s,a) = high_value

function createOptimisticAgent(optimism = 100) {
    const Q = {};  // Q-table
    
    function getQ(state, action) {
        const key = `${state},${action}`;
        if (!(key in Q)) {
            Q[key] = optimism;  // Optimistic init
        }
        return Q[key];
    }
    
    // ... rest of Q-learning
}
```

**Why it works:**
- Encourages systematic exploration
- Every action looks good initially
- Agent tries all options before settling
- Converges to true values

**Best value:**
- Set optimism = max possible cumulative reward
- For slalom: ~800 (8 flags Ã— 100)

---

### **3. Two-Stage Epsilon Annealing**

```javascript
function getEpsilon(episode, epsilon0 = 0.5) {
    if (episode < 100) {
        // Stage 1: Aggressive exploration
        return epsilon0 * Math.pow(0.95, episode / 10);
    } else if (episode < 300) {
        // Stage 2: Moderate exploration
        return 0.1 * Math.pow(0.98, (episode - 100) / 10);
    } else {
        // Stage 3: Fine-tuning
        return Math.max(0.01, 0.02 * Math.pow(0.995, episode - 300));
    }
}

// Episode 0: Îµ = 0.50 (explore)
// Episode 100: Îµ = 0.10 (transition)
// Episode 300: Îµ = 0.02 (exploit)
// Episode 600: Îµ = 0.01 (converged)
```

---

### **4. Reward Shaping (Potential-Based)**

**Problem:** Sparse rewards (only at flags)

**Solution:** Add shaping without changing optimal policy

```javascript
// Define potential function
function potential(state) {
    const flag = state.flags[state.currentFlag];
    if (!flag) return 0;
    
    const dist = Math.hypot(state.x - flag.x, state.y - flag.y);
    return -dist / 400;  // Normalize to [-1, 0]
}

// Shaped reward
function shapedReward(state, action, nextState, originalReward, gamma = 0.95) {
    const F = gamma * potential(nextState) - potential(state);
    return originalReward + F;
}
```

**Theoretical guarantee:**
- Optimal policy unchanged (Ng et al., 1999)
- Dense feedback accelerates learning
- No manual reward engineering

---

### **5. Experience Replay (Lightweight)**

```javascript
const replayBuffer = [];
const maxBufferSize = 1000;

function storeExperience(state, action, reward, nextState) {
    replayBuffer.push({ state, action, reward, nextState });
    if (replayBuffer.length > maxBufferSize) {
        replayBuffer.shift();
    }
}

function replayExperiences(agent, n = 5) {
    // After each real step, learn from n random past experiences
    for (let i = 0; i < n; i++) {
        if (replayBuffer.length < 10) break;
        
        const idx = Math.floor(Math.random() * replayBuffer.length);
        const exp = replayBuffer[idx];
        
        agent.learn(exp.state, exp.action, exp.reward, exp.nextState);
    }
}
```

**Benefits:**
- Reuse experiences (data efficiency)
- Break correlations
- Faster convergence

**Cost:**
- Memory overhead
- Computational cost

---

### **6. Federation Improvements**

```javascript
// Weighted averaging based on performance
function weightedFederation(clients) {
    const weights = clients.map(c => 
        Math.exp(c.metrics.averageReward)  // Better clients weighted more
    );
    const totalWeight = sum(weights);
    
    // Aggregate Q-tables
    const globalQ = {};
    clients.forEach((client, i) => {
        const w = weights[i] / totalWeight;
        for (let [key, value] of Object.entries(client.agent.Q)) {
            globalQ[key] = (globalQ[key] || 0) + w * value;
        }
    });
    
    return globalQ;
}
```

---

## ðŸŽ¯ Curriculum Learning Approach

### **Why Start with Circle Training?**

**Complexity Comparison:**

| Aspect | Circle | Slalom |
|--------|--------|--------|
| States | 120 | 300 |
| Reward density | Dense (every step) | Sparse (flags only) |
| Success metric | Distance from circle | 8 sequential flags |
| Convergence time | 100-200 episodes | 500-1000 episodes |
| Skill learned | Basic magnet control | Complex navigation |

### **Curriculum Steps:**

1. **Circle Training** (100-200 episodes)
   - Learn: Magnet activation timing
   - Learn: Force magnitude selection
   - Metric: % time within 15px of circle
   - Transfer: Q-table â†’ slalom (partial)

2. **Single Flag** (50 episodes)
   - Learn: Point-to-point navigation
   - Metric: Success rate reaching 1 flag

3. **Two Flags** (100 episodes)
   - Learn: Sequential navigation
   - Metric: Success rate completing 2-flag course

4. **Full Slalom** (200-300 episodes)
   - Learn: Complex 8-flag navigation
   - Metric: Success rate completing all flags

### **Transfer Learning:**

```javascript
// After circle training, initialize slalom Q-table
function transferQTable(circleQ, slomaQ) {
    // Map similar states
    for (let [circleState, action] of Object.entries(circleQ)) {
        // Parse circle state (error, angle, vel)
        // Map to slalom state (relX, relY, vel)
        const slomaState = mapCircleToSlalom(circleState);
        
        // Initialize with circle knowledge
        if (!(slomaState in slomaQ)) {
            slomaQ[slomaState] = circleQ[circleState];
        }
    }
}
```

---

## ðŸ“ˆ Expected Improvements

### **Baseline (Current):**
- Convergence: 500-1000 episodes
- Success rate: 70% after 1000 episodes

### **With Adaptive Î± + Optimistic Init:**
- Convergence: 300-500 episodes (40% faster)
- Success rate: 75% after 500 episodes

### **With Curriculum Learning:**
- Total episodes: 450 (100 circle + 50 single + 100 two-flag + 200 slalom)
- Success rate: 80% after curriculum
- Better generalization

### **With All Techniques:**
- Convergence: 200-300 episodes (60% faster)
- Success rate: 85% after 300 episodes
- More robust policy

---

## ðŸ”§ Implementation Priority

### **High Impact, Easy:**
1. âœ… **Curriculum learning (circle first)** - Created `magnet-circle-training.html`
2. **Optimistic initialization** - 5 lines of code
3. **Two-stage epsilon** - 10 lines of code

### **High Impact, Medium:**
4. **State-visit based Î±** - 20 lines + tracking
5. **Potential-based shaping** - 15 lines

### **Medium Impact, Complex:**
6. **Experience replay** - 50 lines + memory
7. **Weighted federation** - 30 lines

---

## ðŸ“š Theoretical Background

### **Q-Learning Convergence Conditions:**

1. **All (s,a) visited infinitely often**
   - Ensured by Îµ-greedy policy

2. **Learning rate conditions (Robbins-Monro):**
   - Î£ Î±_t = âˆž (learn forever)
   - Î£ Î±_tÂ² < âˆž (stabilize)
   - Example: Î±_t = 1/t^0.6

3. **Bounded rewards:**
   - |r| < âˆž for all transitions
   - Already satisfied

### **Optimal Policy Preservation:**

Potential-based shaping preserves optimal policy if:
```
F(s, s') = Î³ Â· Î¦(s') - Î¦(s)
```
for any potential function Î¦(s).

**Proof intuition:**
- Q-shaping and Q-original differ by state-only term
- argmax_a remains unchanged
- Optimal policy identical

---

## ðŸŽ“ References

- Sutton & Barto (2018): Reinforcement Learning: An Introduction
- Ng et al. (1999): Policy Invariance Under Reward Transformations
- Even-Dar & Mansour (2003): Learning Rates for Q-learning
- Mnih et al. (2015): Human-level control through deep RL (Experience Replay)

---

## ðŸš€ Next Steps

1. Test circle training - verify faster convergence
2. Implement state-visit based Î±
3. Add optimistic initialization
4. Measure improvement
5. Apply to slalom with curriculum

