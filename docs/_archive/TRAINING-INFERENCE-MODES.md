# Training/Inference Mode System

## Overview

The federated RL system now features **complete separation** between training and inference modes, following 2025 RL best practices.

## ğŸ¯ Key Features

### Training Mode (ğŸ§ )
- **Multi-client learning**: 4 agents learning simultaneously
- **Federation**: FedAvg with configurable intervals
- **Live metrics**: Q-table growth, epsilon decay, episodes
- **Model persistence**: Save checkpoints to localStorage or file

### Inference Mode (ğŸ¯)
- **Frozen weights**: Îµ = 0 (greedy policy), Î± = 0 (no learning)
- **Evaluation runner**: N episodes (10/25/50/100)
- **Aggregate metrics**: Success rate, mean reward Â± std, consistency score
- **Single agent**: Clean visualization of evaluation

## ğŸš€ Quick Start

### 1. Training Phase

```javascript
// Training happens automatically when you click "Start"
// The system uses 4 clients learning in parallel
```

**Actions:**
- **â–¶ Start/â¸ Pause**: Control training
- **ğŸ”„ Federate**: Manually trigger federation round
- **ğŸ’¾ Save Checkpoint**: Save current model to localStorage
- **ğŸ“¥ Export**: Download model as JSON file
- **â†» Reset**: Restart training from scratch

**Watch for:**
- Q-table growing (states explored)
- Epsilon decaying (exploration â†’ exploitation)
- Success rate improving

### 2. Switch to Inference

Click the **ğŸ¯ Inference** tab to enter evaluation mode:

1. Training automatically pauses
2. Latest model is saved to localStorage
3. UI switches to single-agent view
4. Inference controls appear

### 3. Run Evaluation

**Choose model source:**
- **Latest Training Session**: Uses last trained model from localStorage
- **Load from File**: Upload a previously exported .json model

**Select test episodes:**
- 10, 25, 50, or 100 episodes

**Click â–¶ Run Evaluation:**
- Agent runs with frozen weights (no learning)
- Progress bar shows completion
- Real-time visualization on canvas
- Metrics update live

### 4. Review Results

After evaluation completes:

```
ğŸ“Š Evaluation Results
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SUCCESS RATE: 87%
43/50 successful

MEAN REWARD: 12.4 Â± 3.2
Ïƒ = 3.18

CONSISTENCY SCORE: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 82%
```

**Metrics explained:**
- **Success Rate**: % of episodes where agent succeeded
- **Mean Reward**: Average reward per episode
- **Std Deviation (Ïƒ)**: Reward variability
- **Consistency**: 1 - (Ïƒ / |mean|), higher = more consistent

**Actions:**
- **ğŸ“Š Export Results**: Download evaluation metrics as JSON
- **â¹ Stop**: Cancel running evaluation

### 5. Back to Training

Click the **ğŸ§  Training** tab:
- Training clients restored
- Can continue training from where you left off
- Or reset and start fresh

## ğŸ“Š Model Persistence

### Three Storage Options

1. **localStorage (automatic)**
   - Saved on mode switch
   - Saved on "Save Checkpoint"
   - Key: `${appName}-latest`
   - âš ï¸ Limited to ~5-10MB

2. **JSON Export (manual)**
   - Click "ğŸ“¥ Export" in training mode
   - Downloads: `app-name-model-timestamp.json`
   - Contains: model weights + metadata

3. **Inference Results Export**
   - Click "ğŸ“Š Export Results" after evaluation
   - Downloads: `app-name-inference-timestamp.json`
   - Contains: summary + per-episode data

## ğŸ—ï¸ Architecture

### Component Structure

```
components/
â”œâ”€â”€ mode-switcher.js      # UI tabs + visibility control
â”œâ”€â”€ inference-mode.js     # Frozen agent + evaluation runner
â”œâ”€â”€ app-template.js       # Orchestrates both modes
â”œâ”€â”€ rl-core.js           # Q-learning algorithms
â””â”€â”€ federated-core.js    # FedAvg + serialization
```

### State Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Save    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Training   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ localStorageâ”‚
â”‚  4 clients  â”‚           â”‚   or File   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                          â”‚
    [Switch]                   Load
       â†“                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Inference  â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ Frozen Agentâ”‚
â”‚  1 client   â”‚           â”‚ (Îµ=0, Î±=0)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€â”€> N episodes â”€â”€> Results
```

## ğŸ§ª Implementation Example

### Minimal Setup

```javascript
import { createFederatedApp } from '../components/app-template.js';

createFederatedApp({
    name: 'My RL Demo',
    numClients: 4,
    canvasWidth: 400,
    canvasHeight: 400,
    
    // RL config
    alpha: 0.1,
    gamma: 0.95,
    epsilon: 0.3,
    
    // Environment
    environment: {
        actions: ['LEFT', 'STAY', 'RIGHT'],
        getState: (state) => `${state.x},${state.y}`,
        step: (state, action) => ({ state, reward, done }),
        reset: () => initialState
    },
    
    // Render
    render: (ctx, state) => {
        // Draw visualization
    }
});

// Mode switching is automatic! ğŸ‰
```

### Custom Mode Behavior

```javascript
createFederatedApp({
    // ... config ...
    
    onClientInit: (client) => {
        // Called when training client initializes
        console.log('Training client created:', client.id);
    },
    
    onEpisodeEnd: (client) => {
        // Called after each training episode
        const rate = client.state.catches / 
                    (client.state.catches + client.state.misses);
        console.log(`Client ${client.id} success rate: ${rate}`);
    },
    
    onFederation: (globalModel, round) => {
        // Called after federation
        console.log(`Round ${round}: ${Object.keys(globalModel).length} states`);
    }
});
```

## ğŸ’¡ Best Practices

### Training
1. **Train until epsilon stabilizes** (~200-500 episodes)
2. **Watch Q-table growth** - should plateau
3. **Save checkpoints periodically** 
4. **Use auto-federation** for consistency

### Inference
1. **Run sufficient episodes** (â‰¥50 for statistical significance)
2. **Compare multiple checkpoints** to find best
3. **Export results** for reproducibility
4. **Check consistency score** - should be >70%

### Model Selection
- High mean reward + high consistency = production ready
- High mean reward + low consistency = needs more training
- Low mean reward = architecture/hyperparameter issue

## ğŸ” Troubleshooting

### "No training data found"
**Solution**: Train first, then switch to inference. Or load a model file.

### localStorage quota exceeded
**Solution**: Use JSON export instead. Clear old data:
```javascript
localStorage.clear();
```

### Inference results inconsistent
**Solution**: 
- Increase test episodes (50 â†’ 100)
- Check if training converged (epsilon near minEpsilon)
- Verify state space isn't too large

### Mode switch doesn't work
**Solution**: Check browser console for errors. Ensure all imports are correct.

## ğŸ“š Related Documentation

- [Component Library Guide](./COMPONENT-LIBRARY-GUIDE.md)
- [Federated RL Comparison](./federated-rl-comparison.md)
- [README](../README.md)

## ğŸ¯ Example Workflow

```bash
# 1. Start training
Click "ğŸ§  Training" tab (default)
Click "â–¶ Start"
Wait ~100-200 episodes

# 2. Save checkpoint
Click "ğŸ’¾ Save Checkpoint"
Note: "Checkpoint saved at 14:32:15"

# 3. Switch to inference
Click "ğŸ¯ Inference" tab

# 4. Run evaluation
Select "Latest Training Session"
Select "50 episodes"
Click "â–¶ Run Evaluation"
Wait for completion

# 5. Review & export
Check success rate, mean reward, consistency
Click "ğŸ“Š Export Results" to save metrics

# 6. Continue training (optional)
Click "ğŸ§  Training" tab
Click "â–¶ Start" to continue
```

---

**Built with 2025 RL best practices** ğŸš€

